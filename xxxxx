import os
import requests
import numpy as np
import pandas as pd
import json
import time
import logging
import optuna
import warnings
from sklearn.exceptions import DataConversionWarning
from datetime import datetime
from tensorflow.keras.models import Sequential, load_model
from pandas.api.types import is_numeric_dtype
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import TensorBoard
from kerastuner.tuners import RandomSearch
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from rich.console import Console
from rich.table import Table

API_KEY = "HOJyZGfniMt33ugFLT7BAWhhBqLYWGLdooXo6VTY"
fib_cols = ['level_0', 'level_236', 'level_382', 'level_50', 'level_618', 'level_786', 'level_100']

headers = {'Authorization': 'Bearer ' + API_KEY}

# initialize logging system here
# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)s: %(message)s',
                    handlers=[logging.FileHandler("OPlog5minH.log"),
                              logging.StreamHandler()])

# Initialize column names to lowercase
high_col = 'high'
low_col = 'low'
close_col = 'close'
volume_col = 'volume'

# Update the numerical_cols to reflect the dynamically determined column names
numerical_cols = [high_col, low_col, close_col, volume_col]

# Function to read API list from file
def read_api_list(filename):
    with open(filename, 'r') as f:
        return [line.strip() for line in f]

# Rich console initialization
console = Console()

# Function to display metrics in a table using Rich
def display_metrics_in_table(metrics):
    table = Table(show_header=True, header_style="bold magenta")
    table.add_column("Metric", style="dim", width=12)
    table.add_column("Value")

    for key, value in metrics.items():
        table.add_row(key, str(value))

    console.print(table)

def predict_and_retrain(model, x_scaled, y_scaled, y_actual, scaler_y, latest_eth_data, tensorboard_callback):
    # Check for NaN inputs and print a warning
    if np.isnan(x_scaled).any():
        logging.warning("Warning: NaN values detected in input data for prediction.")

    y_actual_scaled = scaler_y.transform(y_actual)
    max_retries = 100
    counter = 0
    while counter < max_retries:
        prediction = model.predict(x_scaled)
        real_price = scaler_y.inverse_transform(y_scaled)
        predicted_price = scaler_y.inverse_transform(prediction)
        error = abs(real_price[0][0] - predicted_price[0][0])  # This is the absolute difference
        if error > 0.10:  # If the absolute difference is more than 10 cents
            logging.info(f"Error is greater than $0.10. Retraining the model, attempt {counter + 1}...")
            model.fit(x_scaled, y_actual_scaled, epochs=100, verbose=0, batch_size=64, callbacks=[tensorboard_callback])
            counter += 1
        else:
            break
    return model

def create_tensorboard_callback(experiment_name):
    log_dir = f"logs/{experiment_name}/" + datetime.now().strftime("%Y%m%d-%H%M%S")
    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)
    return tensorboard_callback

def fetch_api_data(api_list):
    api_data = {}
    for url in api_list:
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                if 'result' in data and 'data' in data['result']:
                    df_data = data['result']['data']
                    df = pd.DataFrame(df_data)
                else:
                    df = pd.DataFrame(data)

                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                    df.set_index('timestamp', inplace=True)

                df['Timestamp'] = df.index
                api_data[url] = df
                logging.info(f"Data fetched successfully from {url}")
            else:
                logging.error(f"Failed to fetch data from {url}. Status code: {response.status_code}")
        except Exception as e:
            logging.error(f"Error fetching data from {url}: {e}")
    return api_data

def get_initial_data(n_past: int, max_retries: int = 5) -> pd.DataFrame:
    historical_api_list = read_api_list("ares4.txt")
    url_ohlcv = historical_api_list[0] + str(n_past)
    for _ in range(max_retries):
        try:
            response = requests.get(url_ohlcv, headers=headers)
            response.raise_for_status()
            data = response.json()["result"]["data"]
            initial_data = pd.DataFrame(data)

            for api in historical_api_list[1:]:
                response_additional = requests.get(api, headers=headers)
                additional_data = response_additional.json()["result"]["data"]
                df_additional = pd.DataFrame(additional_data)
                initial_data = pd.concat([initial_data, df_additional], axis=1)

            if len(initial_data) < n_past:
                logging.warning(f"Initial data has less than {n_past} rows. Found {len(initial_data)} rows.")
                continue  # You may want to return None or handle this differently

            return initial_data, datetime.now()
        except requests.exceptions.RequestException as e:
            logging.warning(f"Error occurred: {e}. Retrying...")
            time.sleep(5)

    raise Exception("Max retries exceeded. Please check your internet connection or the status of the API.")

def get_latest_eth_data(previous_data, last_successful_call, api_list, max_retries=5):
    real_time_api_list = read_api_list("ares3.txt")
    url_ohlcv = real_time_api_list[0] + "5"
    for _ in range(max_retries):
        try:
            response = requests.get(url_ohlcv, headers=headers)
            response.raise_for_status()
            data = response.json()["result"]["data"]
            latest_ohlcv_data = pd.DataFrame(data)

            if 'Timestamp' not in latest_ohlcv_data.columns:
                latest_ohlcv_data['Timestamp'] = pd.to_datetime(latest_ohlcv_data['datetime'])

            latest_ohlcv_data.sort_values(by='Timestamp', ascending=True, inplace=True)

            for idx, api in enumerate(api_list[1:]):
                response_additional = requests.get(api, headers=headers)
                additional_data = response_additional.json()["result"]["data"]
                df_additional = pd.DataFrame(additional_data)
                common_cols = df_additional.columns.intersection(latest_ohlcv_data.columns).tolist()
                df_additional = df_additional.drop(columns=common_cols)
                latest_ohlcv_data = pd.concat([latest_ohlcv_data, df_additional], axis=1)

            # Align columns with previous_data
            aligned_latest_data = latest_ohlcv_data.reindex(columns=previous_data.columns)
            aligned_latest_data.fillna(method='ffill', inplace=True)

            return aligned_latest_data, datetime.now()
        except requests.exceptions.RequestException as e:
            logging.warning(f"Error occurred: {e}. Retrying...")
            time.sleep(5)

    time_diff = datetime.now() - last_successful_call
    missed_minutes = int(time_diff.total_seconds() / 60)
    logging.info(f"Missed {missed_minutes} minutes of data. Fetching historical data...")
    missed_data, _ = get_initial_data(missed_minutes)
    return missed_data, datetime.now()

WINDOW_SIZE = 50

def aggregate_data(api_data):
    main_df = api_data[next(iter(api_data))]
    for key, df in api_data.items():
        if key != next(iter(api_data)):
            main_df = main_df.merge(df, left_index=True, right_index=True, how='left', suffixes=('', '_drop'))
            main_df = main_df.drop(main_df.filter(regex='_drop$').columns, axis=1)

    # Impute missing values only for numeric columns
    for col in main_df.select_dtypes(include=[np.number]).columns:
        if main_df[col].isnull().any():
            main_df[col].fillna(main_df[col].median(), inplace=True)

    # For non-numeric columns, use forward-fill or back-fill
    for col in main_df.select_dtypes(exclude=[np.number]).columns:
        main_df[col] = main_df[col].fillna(method='ffill').fillna(method='bfill')

    return main_df

def preprocess_real_time_data(latest_eth_data, previous_data, n_past, scaler_x, scaler_y):
    logging.info(f"Shape of previous_data: {previous_data.shape}")
    logging.info(f"Shape of latest_eth_data: {latest_eth_data.shape}")

    if scaler_x is None or scaler_y is None:
        logging.error("Scaler_x or scaler_y is None, cannot proceed with data transformation")
        return None

    latest_eth_data = latest_eth_data.drop_duplicates()
    previous_data = previous_data.drop_duplicates()

    # Ensure alignment of columns in latest_eth_data with previous_data
    aligned_latest_data = latest_eth_data.reindex(columns=previous_data.columns)

    for col in aligned_latest_data.columns:
        if aligned_latest_data[col].isna().any():
            if is_numeric_dtype(aligned_latest_data[col]):
                # Fill missing numeric values with the median of the previous data
                aligned_latest_data[col].fillna(previous_data[col].median(), inplace=True)
            else:
                # For non-numeric columns, use forward-fill followed by back-fill
                aligned_latest_data[col].fillna(method='ffill', inplace=True)
                aligned_latest_data[col].fillna(method='bfill', inplace=True)

    # Combine the latest data with previous data and select the most recent n_past rows
    input_data = pd.concat([previous_data, aligned_latest_data]).tail(n_past)

    # Scale the numeric columns of the input data
    x_scaled = scaler_x.transform(input_data[numerical_cols])
    x_scaled = x_scaled.reshape((1, n_past, len(numerical_cols)))

    if np.isnan(x_scaled).any():
        logging.warning("Warning: NaN values detected in x_scaled!")

    return x_scaled

def predict_eth_price(input_data, model):
    prediction = model.predict(np.array(input_data))
    return prediction

def custom_loss(y_true, y_pred):
    error = y_true - y_pred
    absolute_error = tf.abs(error)
    squared_error = tf.square(error)
    return tf.where(absolute_error < 0.05, squared_error, squared_error + 0.1 * tf.square(absolute_error - 0.05))

def create_model(n_past, learning_rate=0.005):
    n_features = len(numerical_cols)
    model = Sequential()
    model.add(LSTM(units=512, activation='tanh', return_sequences=True, input_shape=(n_past, n_features)))
    model.add(Dropout(0.3))
    model.add(LSTM(units=256, activation='tanh', return_sequences=False))
    model.add(Dense(units=1, activation='linear'))
    optimizer = Adam(learning_rate=learning_rate, clipnorm=5)
    model.compile(optimizer=optimizer, loss=custom_loss)
    return model

def prepare_training_data(data, scaler_x, scaler_y, window_size):
    X, Y = [], []
    for i in range(len(data) - window_size):
        X.append(data[numerical_cols].iloc[i:i + window_size].values)
        Y.append(data[high_col].iloc[i + window_size])

    if not X:
        logging.error("X is empty after preparing training data.")
        return None, None

    X_array = np.array(X)
    X_scaled = scaler_x.transform(X_array.reshape(-1, X_array.shape[-1])).reshape(X_array.shape)
    Y_scaled = scaler_y.transform(np.array(Y).reshape(-1, 1)).reshape(-1)

    logging.info(f"Prepared training data: X_scaled shape: {X_scaled.shape}, Y_scaled shape: {Y_scaled.shape}")
    return X_scaled, Y_scaled

def build_model(trial):
    # Define the hyperparameter search space using Optuna
    n_features = len(numerical_cols)
    model = Sequential()
    model.add(LSTM(units=trial.suggest_categorical('units_lstm', [64, 128, 256, 512]),
                   activation='tanh', return_sequences=True, input_shape=(WINDOW_SIZE, n_features)))
    model.add(Dropout(trial.suggest_float('dropout_lstm', 0.1, 0.5)))
    model.add(LSTM(units=trial.suggest_categorical('units_dense', [64, 128, 256, 512]), activation='tanh'))
    model.add(Dense(units=1, activation='linear'))

    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)
    optimizer = Adam(learning_rate=learning_rate, clipnorm=5)
    model.compile(optimizer=optimizer, loss=custom_loss)
    return model

def objective(trial, data, scaler_x, scaler_y):
    # Create the LSTM model with hyperparameters
    model = build_model(trial)

    # Prepare training data
    x_train, y_train = prepare_training_data(data, scaler_x, scaler_y, WINDOW_SIZE)

    # Split data into training and validation sets
    split_idx = int(len(x_train) * 0.8)
    x_train_split, x_val_split = x_train[:split_idx], x_train[split_idx:]
    y_train_split, y_val_split = y_train[:split_idx], y_train[split_idx:]

    # Train the model
    history = model.fit(x_train_split, y_train_split, validation_data=(x_val_split, y_val_split),
                        epochs=50, batch_size=64, verbose=0)

    # Return the validation loss
    return np.min(history.history['val_loss'])

def build_model_with_optuna_params(best_params):
    n_features = len(numerical_cols)
    model = Sequential()

    # Check if 'units_lstm' is in best_params, otherwise use a default value
    lstm_units = best_params.get('units_lstm', 128) # Default value is an example
    model.add(LSTM(units=lstm_units,
                   activation='tanh', return_sequences=True,
                   input_shape=(WINDOW_SIZE, n_features)))

    dropout_rate = best_params.get('dropout_lstm', 0.2) # Default value is an example
    model.add(Dropout(dropout_rate))

    dense_units = best_params.get('units_dense', 64) # Default value is an example
    model.add(LSTM(units=dense_units, activation='tanh'))

    model.add(Dense(units=1, activation='linear'))

    learning_rate = best_params.get('learning_rate', 0.001) # Default value is an example
    optimizer = Adam(learning_rate=learning_rate, clipnorm=5)
    model.compile(optimizer=optimizer, loss=custom_loss)

    return model

def train_and_optimize_model_with_optuna(initial_data, study, scaler_x, scaler_y):
    scaler_x = MinMaxScaler()
    scaler_y = MinMaxScaler()
    scaler_x.fit(initial_data[numerical_cols])
    scaler_y.fit(initial_data[[close_col]])

    x_train, y_train = prepare_training_data(initial_data, scaler_x, scaler_y, WINDOW_SIZE)
    if x_train is None or len(x_train) < 10:
        logging.warning("Insufficient data for training and validation. Returning a new model.")
        return create_model(WINDOW_SIZE), scaler_x, scaler_y  # Create a new model

    study.optimize(lambda trial: objective(trial, initial_data, scaler_x, scaler_y), n_trials=4)
    best_params = study.best_params
    with open('best_paramsH.json', 'w') as f:
        json.dump(best_params, f)

    best_model = build_model_with_optuna_params(best_params)
    best_model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[tensorboard_callback], verbose=0)
    return best_model, scaler_x, scaler_y

def save_best_hyperparameters(study, filename):
    with open(filename, 'w') as file:
        json.dump(study.best_params, file)

def load_best_hyperparameters(filename):
    with open(filename, 'r') as file:
        return json.load(file)

if __name__ == "__main__":
    print('Starting Scripts...')
    n_past = WINDOW_SIZE
    tensorboard_callback = create_tensorboard_callback("eth_price_prediction5minH")
    warnings.filterwarnings(action='ignore', category=DataConversionWarning)
    warnings.filterwarnings('ignore')

    # Read and aggregate historical data
    historical_api_list = read_api_list("ares4.txt")
    aggregated_data = fetch_api_data(historical_api_list)
    if not aggregated_data:
        logging.error("No data fetched from historical APIs.")
        exit(1)

    historical_data = aggregate_data(aggregated_data)
    if historical_data.empty or historical_data.isnull().values.any():
        logging.error("Historical data is empty or contains NaN. Exiting...")
        exit(1)

    # Initialize scalers and prepare training data
    scaler_x = MinMaxScaler()
    scaler_y = MinMaxScaler()
    scaler_x.fit(historical_data[numerical_cols])
    scaler_y.fit(historical_data[[high_col]])
    x_train, y_train = prepare_training_data(historical_data, scaler_x, scaler_y, WINDOW_SIZE)

    # Load or train the model
    weights_path = 'initial_optimized_weightsH.hdf5'
    hyperparams_path = 'best_hyperparamsH.json'
    if os.path.exists(weights_path) and os.path.exists(hyperparams_path):
        print("Loading previously optimized weights and hyperparameters...")
        best_hyperparams = load_best_hyperparameters(hyperparams_path)
        model = build_model_with_optuna_params(best_hyperparams)
        model.load_weights(weights_path)
    else:
        print("Training model with historical data using Optuna...")
        study = optuna.create_study(direction='minimize')
        study.optimize(lambda trial: objective(trial, historical_data, scaler_x, scaler_y), n_trials=10)
        best_params = study.best_params
        save_best_hyperparameters(study, 'best_hyperparamsH.json')
        model = build_model_with_optuna_params(best_params)
        model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[tensorboard_callback], verbose=0)
        model.save_weights('initial_optimized_weightsH.hdf5')

    # Initialize CSV files if not present
    if not os.path.exists('5minACTH.csv'):
        with open('5minACTH.csv', 'w') as f:
            f.write('Timestamp,Predicted_Price\n')
    if not os.path.exists('predictions5minH.csv'):
        with open('predictions5minH.csv', 'w') as f:
            f.write('Timestamp,Real_Price,Predicted_Price,Actual_Price_After_5min,Actual_Timestamp_After_5min\n')

    # Main loop
    previous_data = historical_data
    last_successful_call = datetime.now()
    prediction_counter = 0
    start_time = time.time()

    logging.info("Generating initial prediction...")
    initial_x_scaled = preprocess_real_time_data(previous_data, previous_data, n_past, scaler_x, scaler_y)
    initial_prediction = predict_eth_price(initial_x_scaled, model)
    initial_predicted_price = scaler_y.inverse_transform(initial_prediction)
    logging.info(f"Initial Predicted Ethereum Price: {initial_predicted_price[0][0]}")

    while True:
        api_list = read_api_list("ares3.txt")
        logging.info("\nFetching latest Ethereum price and data...")
        latest_eth_data, _ = get_latest_eth_data(previous_data, last_successful_call, api_list)

        time.sleep(300)

        latest_eth_data_after_5min, _ = get_latest_eth_data(previous_data, last_successful_call, api_list)
        if not latest_eth_data_after_5min.empty:
            actual_price_after_5min = latest_eth_data_after_5min[high_col].iloc[-1]
            actual_timestamp_after_5min = latest_eth_data_after_5min['Timestamp'].iloc[-1]
        else:
            logging.warning("No new data fetched after 5 minutes. Skipping retraining.")
            continue

        if latest_eth_data.empty:
            print("No new data fetched. Skipping this iteration.")
            continue

        aggregated_data = fetch_api_data(api_list)
        new_data = aggregate_data(aggregated_data)

        combined_data = pd.concat([previous_data, new_data], ignore_index=True)
        previous_data = combined_data.iloc[-n_past:]

        x_scaled = preprocess_real_time_data(latest_eth_data, previous_data, n_past, scaler_x, scaler_y)

        prediction = predict_eth_price(x_scaled, model)
        predicted_price = scaler_y.inverse_transform(prediction)
        logging.info(f"Current Prediction: {predicted_price[0][0]}")

        prediction_counter += 1

        real_high_price = latest_eth_data[high_col].iloc[-1]

        if pd.isna(real_high_price):
            print("Encountered NaN value in real_high_price. Skipping this iteration.")
            continue

        # Check if timestamp value is available from latest_eth_data
        timestamp_value = latest_eth_data.get('Timestamp')
        if timestamp_value is not None:
            with open('5minACTH.csv', 'a') as f:
                f.write(f"{timestamp_value.iloc[0]},{predicted_price[0][0]}\n")
            with open('predictions5minH.csv', 'a') as f:
                f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')},{real_high_price},{predicted_price[0][0]},{actual_price_after_5min},{actual_timestamp_after_5min}\n")

        # Calculate elapsed time since the start of the loop
        elapsed_time = time.time() - start_time
        sleep_time = max(300 - elapsed_time, 0)
        logging.info(f"Waiting for {sleep_time:.2f} seconds before the next iteration...")
        time.sleep(sleep_time)
