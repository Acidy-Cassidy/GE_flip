import os
import requests
import numpy as np
import pandas as pd
import time
import logging
import optuna
import warnings
from sklearn.exceptions import DataConversionWarning
from datetime import datetime, timedelta
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import TensorBoard
from kerastuner.tuners import RandomSearch
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf

API_KEY = "HOJyZGfniMt33ugFLT7BAWhhBqLYWGLdooXo6VTY"
fib_cols = ['level_0', 'level_236', 'level_382', 'level_50', 'level_618', 'level_786', 'level_100']

headers = {'Authorization': 'Bearer ' + API_KEY}

# initialize logging system here
# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)s: %(message)s',
                    handlers=[logging.FileHandler("OPlog10minH.log"),
                              logging.StreamHandler()])

# Initialize column names to lowercase
high_col = 'high'
low_col = 'low'
close_col = 'close'
volume_col = 'volume'

# Update the numerical_cols to reflect the dynamically determined column names
numerical_cols = ['open', 'high', 'low', 'close', 'volume', 'open_interest', 'taker_buy_volume', 'taker_sell_volume', 'taker_buy_ratio', 'taker_sell_ratio', 'taker_buy_sell_ratio', 'funding_rates', 'velocity_supply_total', 'addresses_count_active', 'addresses_count_sender', 'addresses_count_receiver']

def read_api_list(filename="/opt/ares3.txt"):
    with open(filename, 'r') as f:
        return [line.strip() for line in f]

def predict_and_retrain(model, x_scaled, y_scaled, y_actual, scaler_y, latest_eth_data, tensorboard_callback):
    # Check for NaN inputs and print a warning
    if np.isnan(x_scaled).any():
        logging.warning("Warning: NaN values detected in input data for prediction.")

    y_actual_scaled = scaler_y.transform(y_actual)
    max_retries = 100
    counter = 0
    while counter < max_retries:
        prediction = model.predict(x_scaled)
        real_price = scaler_y.inverse_transform(y_scaled)
        predicted_price = scaler_y.inverse_transform(prediction)
        error = abs(real_price[0][0] - predicted_price[0][0])  # This is the absolute difference
        if error > 0.10:  # If the absolute difference is more than 10 cents
            logging.info(f"Error is greater than $0.10. Retraining the model, attempt {counter + 1}...")
            model.fit(x_scaled, y_actual_scaled, epochs=100, verbose=0, batch_size=64, callbacks=[tensorboard_callback])
            counter += 1
        else:
            break
    return model

def create_tensorboard_callback(experiment_name):
    log_dir = f"logs/{experiment_name}/" + datetime.now().strftime("%Y%m%d-%H%M%S")
    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)
    return tensorboard_callback

def fetch_api_data(api_list):
    api_data = {}
    for url in api_list:
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                # Check for nested structure and extract data if needed
                if 'result' in data and 'data' in data['result']:
                    df_data = data['result']['data']
                    df = pd.DataFrame(df_data)
                else:
                    df = pd.DataFrame(data)

                if 'timestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                    df.set_index('timestamp', inplace=True)

                # Add a 'Timestamp' column
                df['Timestamp'] = df.index

                api_data[url] = df
                logging.warning(f"\nData from {url}:")
                print(df.head())  # Print the top few rows of the DataFrame
            else:
                logging.warning(f"Failed to fetch data from {url}. Status code: {response.status_code}")
        except Exception as e:
            logging.warning(f"Error fetching data from {url}: {e}")
    return api_data

def get_initial_data(n_past: int, max_retries: int = 5) -> pd.DataFrame:
    # Fetch OHLCV data from the new API
    url_ohlcv = api_list[0] + str(n_past)

    for _ in range(max_retries):
        try:
            response = requests.get(url_ohlcv, headers=headers)
            response.raise_for_status()
            data = response.json()["result"]["data"]
            initial_data = pd.DataFrame(data)

            # Fetch additional data from the other APIs (excluding the first one) and augment the dataframe
            for api in api_list[1:]:
                response_additional = requests.get(api, headers=headers)
                additional_data = response_additional.json()["result"]["data"]
                df_additional = pd.DataFrame(additional_data)
                initial_data = pd.concat([initial_data, df_additional], axis=1)

            # Ensure fetched data has at least n_past rows
            if len(initial_data) < n_past:
                raise ValueError(f"Expected initial_data to have at least {n_past} rows, but it has {len(initial_data)} rows.")

            return initial_data, datetime.now()
        except requests.exceptions.RequestException as e:
            logging.warning(f"Error occurred: {e}. Retrying...")
            time.sleep(5)
    raise Exception("Max retries exceeded. Please check your internet connection or the status of the API.")

def get_latest_eth_data(previous_data: pd.DataFrame, last_successful_call: datetime, max_retries: int = 5):
    # Fetch OHLCV data for the last 5 minutes from the new API
    url_ohlcv = api_list[0] + "5"  # Using the first API for OHLCV data

    for _ in range(max_retries):
        try:
            response = requests.get(url_ohlcv, headers=headers)
            response.raise_for_status()
            data = response.json()["result"]["data"]
            latest_ohlcv_data = pd.DataFrame(data)

            # Ensure the 'Timestamp' column exists in the latest_ohlcv_data
            if 'Timestamp' not in latest_ohlcv_data.columns:
                latest_ohlcv_data['Timestamp'] = pd.to_datetime(latest_ohlcv_data['datetime'])

            # Sort the data by Timestamp in descending order (most recent first)
            latest_ohlcv_data.sort_values(by='Timestamp', ascending=True, inplace=True)

            # Fetch additional data from the other APIs (excluding the first one) and augment the dataframe
            for idx, api in enumerate(api_list[1:]):
                response_additional = requests.get(api, headers=headers)
                additional_data = response_additional.json()["result"]["data"]
                df_additional = pd.DataFrame(additional_data)
                # Drop duplicate columns (like 'datetime') from df_additional
                common_cols = df_additional.columns.intersection(latest_ohlcv_data.columns).tolist()
                df_additional = df_additional.drop(columns=common_cols)
                latest_ohlcv_data = pd.concat([latest_ohlcv_data, df_additional], axis=1)  # Augment data

            return latest_ohlcv_data, datetime.now()
        except requests.exceptions.RequestException as e:
            logging.warning(f"Error occurred: {e}. Retrying...")
            time.sleep(5)

    # Fallback in case of failure to fetch data
    time_diff = datetime.now() - last_successful_call
    missed_minutes = int(time_diff.total_seconds() / 60)
    logging.info(f"Missed {missed_minutes} minutes of data. Fetching historical data...")
    missed_data, _ = get_initial_data(missed_minutes)
    return missed_data, datetime.now()

WINDOW_SIZE = 50

def aggregate_data(api_data):
    # Assuming the first API data is the primary data source
    main_df = api_data[next(iter(api_data))]

    for key, df in api_data.items():
        if key != next(iter(api_data)):  # To skip the first (primary) API data
            # Merge the data on the common index (timestamp)
            main_df = main_df.merge(df, left_index=True, right_index=True, how='left', suffixes=('', '_drop'))
            main_df = main_df.drop(main_df.filter(regex='_drop$').columns, axis=1)
            print(f"\nData after merging with {key}:")
            print(main_df.head())  # Print the top few rows of the DataFrame after each merge

    return main_df

def preprocess_real_time_data(latest_eth_data, previous_data, n_past, scaler_x, scaler_y):
    logging.info(f"Shape of previous_data: {previous_data.shape}")
    logging.info(f"Shape of latest_eth_data: {latest_eth_data.shape}")

    # Validate uniqueness of the index for debugging
    logging.info(f"Unique indices for previous_data: {previous_data.index.nunique()}")
    logging.info(f"Total rows in previous_data: {len(previous_data)}")
    logging.info(f"Unique indices for latest_eth_data: {latest_eth_data.index.nunique()}")
    logging.info(f"Total rows in latest_eth_data: {len(latest_eth_data)}")

    # Drop duplicate rows in both dataframes
    latest_eth_data = latest_eth_data.drop_duplicates()
    previous_data = previous_data.drop_duplicates()

    # Handle NaN values: Fill NaN values using interpolation (linear method)
    latest_eth_data = latest_eth_data.interpolate(method='linear')
    previous_data = previous_data.interpolate(method='linear')

    # If there are still NaN values left, fill them with the last valid observation
    latest_eth_data.fillna(method='ffill', inplace=True)
    previous_data.fillna(method='ffill', inplace=True)

    # If there are still NaN values left (this handles the case where the first few rows are NaN),
    # fill them with the next valid observation
    latest_eth_data.fillna(method='bfill', inplace=True)
    previous_data.fillna(method='bfill', inplace=True)

    # Ensure that both dataframes have the same columns
    for col in latest_eth_data.columns:
        if col not in previous_data.columns:
            previous_data[col] = np.nan
    for col in previous_data.columns:
        if col not in latest_eth_data.columns:
            latest_eth_data[col] = np.nan

    # Reset the index and then concatenate
    input_data = pd.concat([previous_data.reset_index(drop=True), latest_eth_data.reset_index(drop=True)], axis=0, ignore_index=True)
    logging.info(f"Shape after concatenation: {input_data.shape}")

    # Keep only the last `n_past` rows
    input_data = input_data.tail(n_past)

    if len(input_data) != n_past:
        raise ValueError(f"Expected input_data to have {n_past} rows, but it has {len(input_data)} rows.")

    # Ensure the data has the correct columns for scaling
    for col in numerical_cols:
        if col not in input_data.columns:
            input_data[col] = 0

    x_scaled = scaler_x.transform(input_data[numerical_cols])
    x_scaled = x_scaled.reshape((1, n_past, len(numerical_cols)))

    # Check for NaN values in x_scaled
    if np.isnan(x_scaled).any():
        logging.warning("Warning: NaN values detected in x_scaled!")

    print("Initial data shape: ", input_data.shape)
    print("Numerical columns: ", numerical_cols)
    return x_scaled

def predict_eth_price(input_data, model):
    prediction = model.predict(np.array(input_data))
    return prediction

def custom_loss(y_true, y_pred):
    error = y_true - y_pred
    absolute_error = tf.abs(error)
    squared_error = tf.square(error)
    return tf.where(absolute_error < 0.05, squared_error, squared_error + 0.1 * tf.square(absolute_error - 0.05))

def create_model(n_past, learning_rate=0.005):
    n_features = len(numerical_cols)
    model = Sequential()
    model.add(LSTM(units=512, activation='tanh', return_sequences=True, input_shape=(n_past, n_features)))
    model.add(Dropout(0.3))
    model.add(LSTM(units=256, activation='tanh', return_sequences=False))
    model.add(Dense(units=1, activation='linear'))
    optimizer = Adam(learning_rate=learning_rate, clipnorm=5)
    model.compile(optimizer=optimizer, loss=custom_loss)
    return model

def prepare_training_data(data, scaler_x, scaler_y, window_size):
    X, Y = [], []
    for i in range(len(data) - window_size + 1):  # Allows for exactly window_size data
        X.append(data[numerical_cols].iloc[i:i + window_size].values)
        Y.append(data[high_col].iloc[i + window_size - 1])

    if len(X) == 0 or len(Y) == 0:
        raise ValueError("Insufficient data to create training datasets.")

    X, Y = np.array(X), np.array(Y)
    X = scaler_x.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
    Y = scaler_y.transform(Y.reshape(-1, 1)).reshape(-1)
    return X, Y

def build_model(trial):
    # Define the hyperparameter search space using Optuna
    n_features = len(numerical_cols)
    model = Sequential()
    model.add(LSTM(units=trial.suggest_categorical('units_lstm', [64, 128, 256, 512]),
                   activation='tanh', return_sequences=True, input_shape=(WINDOW_SIZE, n_features)))
    model.add(Dropout(trial.suggest_float('dropout_lstm', 0.1, 0.5)))
    model.add(LSTM(units=trial.suggest_categorical('units_dense', [64, 128, 256, 512]), activation='tanh'))
    model.add(Dense(units=1, activation='linear'))

    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)
    optimizer = Adam(learning_rate=learning_rate, clipnorm=5)
    model.compile(optimizer=optimizer, loss=custom_loss)
    return model

def objective(trial, initial_data, scaler_x, scaler_y):
    # Define the hyperparameters using trial.suggest_*
    n_units = trial.suggest_categorical('units', [32, 64, 128, 256, 512])
    dropout_rate = trial.suggest_float('dropout', 0, 0.5)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)

    # Create model using the suggested hyperparameters
    model = Sequential()
    model.add(LSTM(units=n_units, activation='tanh', return_sequences=True, input_shape=(WINDOW_SIZE, len(numerical_cols))))
    model.add(Dropout(dropout_rate))
    model.add(LSTM(units=n_units, activation='tanh'))
    model.add(Dense(units=1, activation='linear'))
    optimizer = Adam(learning_rate=learning_rate, clipnorm=5)
    model.compile(optimizer=optimizer, loss=custom_loss)

    # Fit the model
    x_train, y_train = prepare_training_data(initial_data, scaler_x, scaler_y, WINDOW_SIZE)
    history = model.fit(x_train, y_train, epochs=10, verbose=0, batch_size=64, validation_split=0.2)

    # You can access the validation loss from the history object and return it as the objective to minimize
    val_loss = np.min(history.history['val_loss'])
    return val_loss

def build_model_with_optuna_params(best_params):
    n_features = len(numerical_cols)
    model = Sequential()

    # Check if 'units_lstm' is in best_params, otherwise use a default value
    lstm_units = best_params.get('units_lstm', 128) # Default value is an example
    model.add(LSTM(units=lstm_units,
                   activation='tanh', return_sequences=True,
                   input_shape=(WINDOW_SIZE, n_features)))

    dropout_rate = best_params.get('dropout_lstm', 0.2) # Default value is an example
    model.add(Dropout(dropout_rate))

    dense_units = best_params.get('units_dense', 64) # Default value is an example
    model.add(LSTM(units=dense_units, activation='tanh'))

    model.add(Dense(units=1, activation='linear'))

    learning_rate = best_params.get('learning_rate', 0.001) # Default value is an example
    optimizer = Adam(learning_rate=learning_rate, clipnorm=5)
    model.compile(optimizer=optimizer, loss=custom_loss)

    return model

def train_and_optimize_model_with_optuna(initial_data, study):
    scaler_x = MinMaxScaler()
    scaler_y = MinMaxScaler()
    scaler_x.fit(initial_data[numerical_cols])
    scaler_y.fit(initial_data[[high_col]])

    x_train, y_train = prepare_training_data(initial_data, scaler_x, scaler_y, WINDOW_SIZE)

    # Pass all necessary arguments to the objective function
    study.optimize(lambda trial: objective(trial, initial_data, scaler_x, scaler_y), n_trials=10)

    best_params = study.best_params
    best_model = build_model_with_optuna_params(best_params)

    # Retrain the best model
    best_model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[tensorboard_callback], verbose=0)

    return best_model, scaler_x, scaler_y

if __name__ == "__main__":
    print('Starting Scripts...')
    n_past = WINDOW_SIZE

    # Tensorboard initialization
    tensorboard_callback = create_tensorboard_callback("eth_price_prediction10minH")
    warnings.filterwarnings(action='ignore', category=DataConversionWarning)
    warnings.filterwarnings('ignore')
    # Fetching initial large historical data for training
    logging.info("Fetching large historical data for initial training...")
    api_list = read_api_list()
    aggregated_data = fetch_api_data(api_list)
    historical_data = aggregate_data(aggregated_data)

    # Initialize Optuna study
    study = optuna.create_study(direction='minimize')

    # Training and optimizing the model with historical data using Optuna
    logging.info("Training and optimizing model with historical data using Optuna...")
    model, scaler_x, scaler_y = train_and_optimize_model_with_optuna(historical_data, study)

    # Save initial optimized model weights
    model.save_weights('initial_optimized_weightsH.hdf5')

    # File initialization
    if not os.path.exists('10minACTH.csv'):
        with open('10minACTH.csv', 'w') as f:
            f.write('Timestamp,Predicted_Price\n')
    if not os.path.exists('predictions10minH.csv'):
        with open('predictions10minH.csv', 'w') as f:
            f.write('Timestamp,Real_Price,Predicted_Price,Actual_Price_After_10min,Actual_Timestamp_After_10min\n')

    previous_data = historical_data
    last_successful_call = datetime.now()
    prediction_counter = 0  # Initialize prediction counter
    start_time = time.time()

    while True:
        logging.info("\nFetching latest Ethereum price and data...")
        latest_eth_data, last_successful_call = get_latest_eth_data(previous_data, last_successful_call)

        if latest_eth_data.empty:
            print("No new data fetched. Skipping this iteration.")
            continue

        logging.info(f"latest_eth_data:\n", latest_eth_data)

        # Fetch new aggregated data
        aggregated_data = fetch_api_data(api_list)
        new_data = aggregate_data(aggregated_data)

        # Combine the previous data and new data
        combined_data = pd.concat([previous_data, new_data], ignore_index=True)

        # Ensure we have only n_past rows for the next iteration
        previous_data = combined_data.iloc[-n_past:]

        logging.info("Preprocessing real-time data...")
        x_scaled = preprocess_real_time_data(latest_eth_data, previous_data, n_past, scaler_x, scaler_y)

        logging.info("Predicting Ethereum price...")
        prediction_start_time = datetime.now()
        prediction = predict_eth_price(x_scaled, model)
        predicted_price = scaler_y.inverse_transform(prediction)
        prediction_counter += 1  # Increment prediction counter

        real_close_price = latest_eth_data[high_col].iloc[-1]

        if pd.isna(real_close_price):
            print("Encountered NaN value in real_close_price. Skipping this iteration.")
            continue

        logging.info(f"Real Ethereum Close Price: {real_close_price}")
        logging.info(f"Predicted Ethereum Price: {predicted_price}")

        logging.info("Waiting for 10 minutes before retraining...")
        time.sleep(600)  # Wait for 5 minutes

        # Fetch the latest Ethereum data again after 5 minutes
        actual_timestamp_after_10min = (prediction_start_time + timedelta(minutes=5)).strftime('%Y-%m-%d %H:%M:%S')
        latest_eth_data, _ = get_latest_eth_data(previous_data, last_successful_call)
        actual_price_after_10min = latest_eth_data[high_col].iloc[-1]
        y_scaled = scaler_y.transform([[actual_price_after_10min]])

        logging.info("Training the model on the new data...")
        model = predict_and_retrain(model, x_scaled, y_scaled, [[actual_price_after_10min]], scaler_y, latest_eth_data, tensorboard_callback)
        model.save_weights('best_weights_pred10minH.hdf5')

        timestamp_value = latest_eth_data.get('Timestamp')
        if timestamp_value is not None:
            with open('10minACTH.csv', 'a') as f:
                f.write(f"{timestamp_value.iloc[0]},{predicted_price[0][0]}\n")
            with open('predictions10minH.csv', 'a') as f:
                f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')},{real_close_price},{predicted_price[0][0]},{actual_price_after_10min},{actual_timestamp_after_10min}\n")
        elapsed_time = time.time() - start_time
        sleep_time = max(600 - elapsed_time, 0)
        logging.info(f"Waiting for {sleep_time:.2f} seconds before the next iteration...")
        time.sleep(sleep_time)
